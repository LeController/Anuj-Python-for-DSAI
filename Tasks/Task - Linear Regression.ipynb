{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f815427c-e205-4a09-99eb-86204f403529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if error < tol: # MSE in yhat and y is less than some threshold\n",
    "#     break\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import inv # inverse matrix\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "intercept = np.ones((X_train.shape[0],1))\n",
    "X_train = np.concatenate((intercept, X_train), axis=1) # axis 1 is COLUMN\n",
    "\n",
    "intercept = np.ones((X_test.shape[0],1))\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "59e285c5-10e9-468a-aa49-f93ec31f313a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef closed_form(X, y):\\n    return inv(X.T @ X) @ X.T @ y\\n\\ntheta = closed_form(X_train, y_train)\\n# print(theta)\\nyhat = X_test @ theta # multiply testing dataset with weights (yhat = predicted y)\\nprint(\"X_test.shape =\", X_test.shape, \"theta.shape =\", theta.shape, \"yhat.shape =\", yhat.shape)\\n# print(y_test.shape)\\n# print(yhat[56:60])\\n# print(y_test[56:60])\\n\\nmse = ((y_test - yhat)**2).sum()/X_test.shape[0]\\nprint(mse) # aka LOSS\\n'"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def closed_form(X, y):\n",
    "    return inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "theta = closed_form(X_train, y_train)\n",
    "# print(theta)\n",
    "yhat = X_test @ theta # multiply testing dataset with weights (yhat = predicted y)\n",
    "print(\"X_test.shape =\", X_test.shape, \"theta.shape =\", theta.shape, \"yhat.shape =\", yhat.shape)\n",
    "# print(y_test.shape)\n",
    "# print(yhat[56:60])\n",
    "# print(y_test[56:60])\n",
    "\n",
    "mse = ((y_test - yhat)**2).sum()/X_test.shape[0]\n",
    "print(mse) # aka LOSS\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d93e4d75-2d2a-4278-a857-297eb6d4f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, method, alpha=0.0001, max_iter=1000, previous_loss=9999, threshold=0.0001, mini_batch=20):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.previous_loss = previous_loss\n",
    "        self.threshold = threshold\n",
    "        self.method = method\n",
    "        self.mini_batch = mini_batch\n",
    "    \n",
    "    def gradient_descent(self, X_train, y_train):\n",
    "        self.theta = np.zeros(X_train.shape[1])\n",
    "        \n",
    "        if self.method == \"batch\":\n",
    "            for i in range(self.max_iter):\n",
    "                self.yhat = h_theta(X_train, self.theta)\n",
    "                loss = mse(self.yhat, y_train)\n",
    "                loss_difference = self.previous_loss - loss\n",
    "                if loss_difference < self.threshold:\n",
    "                    iter_stop = i\n",
    "                    print(\"ITERATION STOPPED AT\", iter_stop)\n",
    "                    break\n",
    "                self.previous_loss = loss\n",
    "                self.error = self.yhat - y_train\n",
    "                self.grad = gradient(X_train, self.error)\n",
    "                self.theta = self.theta - self.alpha*self.grad  \n",
    "            print(\"MSE = \",loss)\n",
    "            \n",
    "        if self.method == \"sto\":\n",
    "            for i in range(self.max_iter):\n",
    "                random_sample = random.randint(0, (len(X_train)-1))\n",
    "                X_train_sample = X_train[random_sample].reshape(1,-1)\n",
    "                self.yhat = h_theta(X_train_sample, self.theta)\n",
    "                self.error = self.yhat - y_train[random_sample]\n",
    "                #print(X_train_sample.shape, \"AND\", self.error.shape)\n",
    "                self.grad = gradient(X_train_sample, self.error)\n",
    "                self.theta = self.theta - self.alpha*self.grad\n",
    "            loss = mse(self.yhat, y_train[random_sample])\n",
    "            print(\"MSE = \",loss)\n",
    "            \n",
    "        if self.method == \"mini\":\n",
    "            count = 0 \n",
    "            for i in range(self.max_iter):\n",
    "                X_train_sample = X_train[count:count+self.mini_batch]\n",
    "                y_train_sample = y_train[count:count+self.mini_batch]\n",
    "                count += self.mini_batch\n",
    "                if count >= len(X_train):\n",
    "                    count = 0\n",
    "                self.yhat = h_theta(X_train_sample, self.theta)\n",
    "                self.error = self.yhat - y_train_sample\n",
    "                #print(X_train_sample.shape, \"AND\", self.error.shape)\n",
    "                self.grad = gradient(X_train_sample, self.error)\n",
    "                self.theta = self.theta - self.alpha*self.grad\n",
    "            loss = mse(self.yhat, y_train_sample)\n",
    "            print(\"MSE = \",loss)\n",
    "            \n",
    "        #self.yhat = h_theta(X_test, self.theta)\n",
    "        #self.mse = mse(self.yhat, y_test)\n",
    "\n",
    "def h_theta(X_train, theta):\n",
    "    return X_train @ theta\n",
    "\n",
    "def mse(yhat, y_test):\n",
    "    return ((yhat - y_test)**2).sum() / yhat.shape[0]\n",
    "\n",
    "def gradient(X_train, error):\n",
    "    return error @ X_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "8bcb6d83-fd9a-4899-ae10-62b051cd8a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION STOPPED AT 137\n",
      "MSE =  24.59144676113063\n"
     ]
    }
   ],
   "source": [
    "boston = LinearRegression(method=\"batch\", max_iter = 1000, threshold=0.01)\n",
    "boston.gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "a2cbcac2-7ea1-438f-a974-ea6dd0014aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  392.48275441057757\n"
     ]
    }
   ],
   "source": [
    "boston_sto = LinearRegression(method=\"sto\", max_iter=9999)\n",
    "boston_sto.gradient_descent(X_train, y_train)\n",
    "# I think this is working but MSE can be very high, even with 9999 iterations ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "df7bfcbe-4578-46f6-a56d-635c387a84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  24.601436999956984\n"
     ]
    }
   ],
   "source": [
    "boston_mini = LinearRegression(method=\"mini\", max_iter=137, mini_batch=354)\n",
    "boston_mini.gradient_descent(X_train, y_train)\n",
    "# USING THE SAME BATCH SIZE AS X_train.shape[0] gives the same MSE as batch gradient descent from above! IT WORKS!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonDSAI",
   "language": "python",
   "name": "pythondsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
